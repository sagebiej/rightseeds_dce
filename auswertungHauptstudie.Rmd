---
title: "Supplementary material for Consumers' preferences for commons-based and open-source produce: A discrete choice experiment with directional information manipulations."
author: " "
date: "15/06/2023"

output:
  bookdown::html_document2
editor_options: 
  chunk_output_type: console
---


This script analyses the data collected from the main survey of the paper "Consumers' preferences for commons-based and open-source tomatoes: A discrete choice experiment". It includes the second pilot, but not the first pilot. The first pilot cannot be used because attributes etc changed.

The script includes all relevant steps from preparation of raw data to final models. Readers can use the data and code.

The preregistration can be found here https://aspredicted.org/blind.php?x=K3X_S4Z


# Prepare Data


<details>
  <summary>Prepare Data</summary>


First, load packages and user written functions.

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library("flextable")
library("dplyr")          
library("evd")           
library("boot")
library("apollo")
library("sjlabelled")
library("readxl")
library("httr")
library("ggplot2")
library("tidyr")
library("magrittr")
library("psych")
library("kableExtra")
library("tidylog")
library("texreg")
library("stringr")



source("helpfunctions.R")


No_draws =1500

```

```{r}
rightseeds_live_pages <- read_excel("data/mainstudy/rightseeds_live_pages.xlsx")
```



```{r prep}


# read in all datasets and merge

dce_data <- bind_rows(read_excel("data/mainstudy/rightseeds_live_dce_gg.xlsx"), read_excel("data/mainstudy/rightseeds_live_dce_os.xlsx") ,.id = "gg") %>%  ## gg: 1= gg 2 = oc
  left_join(read_excel("data/mainstudy/rightseeds_live_covariates.xlsx"), by ="RID") #%>% 
  #select(1:18, info_text)


dce_dict<- read_excel("data/mainstudy/rightseeds_live_dce_os.xlsx" , sheet = "dictionary")
cov_dict<- read_excel("data/mainstudy/rightseeds_live_covariates.xlsx" , sheet = "dictionary")


  

  # Merge with time stamp data to identify speeders 
  
  database2 <-  dce_data%>% 
   #filter(!is.na(pref1)) %>% 
    filter(!is.na(q28), q8_16==4|q8_16 %in% c(4,6) ) %>%   
left_join(rightseeds_live_pages, by="RID") %>% 
  mutate(duration_lastp = PAGE_SUBMIT_30-PAGE_DISPLAY_30 ,
         duration_lastp_imputed= if_else(is.na(duration_lastp), mean(duration_lastp, na.rm=TRUE),duration_lastp ),
         DURATION_ALL= PAGE_DISPLAY_30+duration_lastp_imputed,
         threshold=1/3*mean(DURATION_ALL)) %>% 
  filter(DURATION_ALL>threshold)  %>% 
     mutate(Female=case_when(q20==2 ~ 1,   TRUE ~0) , Female_s =as.numeric(scale(Female, scale=FALSE)),
            Age_s = as.numeric(scale(q21, scale=FALSE)),
            HigherEdu=case_when(q26_new %in% c(1,2,3,8) ~ 0,   TRUE ~1) , HigherEdu_s =as.numeric(scale(HigherEdu, scale=FALSE)), # higher education as Meisterausbildung und hÃ¶her
            Income=case_when(q28 ==7 ~ mean(q28,na.rm=TRUE),   TRUE ~q28) , Income_s =as.numeric(scale(Income, scale=FALSE)),
       across(matches("._x[4]$ " ) , ~ .x - 1 ) ,
                        oc = as.numeric(gg)-1 ,  # 0= commons 1 =open source
                        winter =as.numeric(jahreszeit)-1 ,  # 0=summer 1 = winter
           across(matches("._x1$") ,  ~ recode(  .x ,`1` = 0.99,`2` = 1.49  , `3` = 1.99 , `4` = 2.49 , `5` = 2.99 , `6` = 3.49 , `7` = 3.99 , `8` = 4.99 , .default =99999)) ,
           a1_EUBIO = a1_x2==2, a1_BIOVER = a1_x2==3 , a2_EUBIO = a2_x2==2, a2_BIOVER = a2_x2==3 ,
           a1_REG = a1_x3==1, a1_DE = a1_x3==2 , a1_ESP = a1_x3==3 , a1_MAR = a1_x3==4 ,a2_REG = a2_x3==1,  a2_DE = a2_x3==2 ,  a2_ESP = a2_x3==3 , a2_MAR = a2_x3==4
    )   %>% 
    as.data.frame()
 table(database2$gg) 
 
 length(unique(database2$RID))

```
</details>



# Descriptive Analysis
We look at the frequencies of attribute levels and correlations between attributes. This should confirm that the design is balanced and orthogonal.

<details>
  <summary>Descriptive Analysis</summary>


```{r design, eval=TRUE}

table(database2$DESIGN_ROW)
  
  ## For all respondents
  table(database2$a1_x1)
  table(database2$a1_x2)
  table(database2$a1_x3)
  table(database2$a1_x4)

  
  #Only for the first respondents
  table(database2$a1_x1[unique(database2$DESIGN_ROW)], useNA = "always")
  table(database2$a1_x2[unique(database2$DESIGN_ROW)], useNA = "always")
  table(database2$a1_x3[unique(database2$DESIGN_ROW)], useNA = "always")
  table(database2$a1_x4[unique(database2$DESIGN_ROW)], useNA = "always")

  

  ## Over all observations
  cor(database2[, 6:15])
  
  ## Only design
  cor(database2[(unique(database2$DESIGN_ROW)), 6:15])
  
  
  
  table(database2$pref1)
  
  #as probabilities
  
  table(database2$pref1)/length(database2$pref1)
  
```

 We plot the choice probabilities conditional on the attribute level. It should confirm the direction and magnitute of our estimates.  


```{r choiceprobs, eval=TRUE}  
    
  barplot(prop.table(table(database2$pref1)), ylim = c(0,1), col=c("cyan1", "cyan3", "darkcyan"))
  
  
  aggregate(as.numeric(pref1)==1 ~ a1_x1 , data=database2 , mean)
  aggregate(as.numeric(pref1)==2 ~ a2_x1 , data=database2 , mean)

  plot(aggregate(as.numeric(pref1)==1 ~ a1_x1 , data=database2 , mean), type="l")
  plot(aggregate(as.numeric(pref1)==2 ~ a2_x1 , data=database2 , mean), type="l")
  
  
  plot(aggregate(as.numeric(pref1)==1 ~ a1_x2 , data=database2 , mean), type="o")
  plot(aggregate(as.numeric(pref1)==2 ~ a2_x2 , data=database2 , mean), type="o")
  
  plot(aggregate(as.numeric(pref1)==1 ~ a1_x3 , data=database2 , mean), type="o")
  plot(aggregate(as.numeric(pref1)==2 ~ a2_x3 , data=database2 , mean), type="o")
  
  plot(aggregate(as.numeric(pref1)==1 ~ a1_x4 , data=database2 , mean), type="o")
  plot(aggregate(as.numeric(pref1)==2 ~ a2_x4 , data=database2 , mean), type="o")
  

  


```


The following table shows how often respondents have chosen which alternatives. For example the 151 in column Alt1 means that 151 respondents have chosen alternative 1 only once in all nine choice situations. 

```{r individualswitching, eval =TRUE}
ind_switch <- database2 %>% 
  group_by(RID, pref1) %>% 
  summarise(n = n()) %>% 
  ungroup %>% 
  group_by(n, pref1) %>% 
  summarise(freq=n()) %>% 
  arrange(pref1) %>% 
  spread(pref1,freq,fill = 0) %>% 
  rename(nchosen=n, Alt1 =2 , Alt2 = 3 , Optout = 4)

kable(ind_switch) %>%  kable_classic_2(full_width = T)
```
</details>

#  Logit Models

The following chunk will estimate the logit models. It is set to `eval = FALSE` as default because estimation takes long. To replicate the analysis, one has to run this chunk only once. The files will be saved as R objects in the local folder.

<details>
  <summary>Logit Models</summary>



```{r estimate logitmodels, eval=FALSE}

source("scripts/clogitmodels.R")
source("scripts/mixlogitmodels.R")
source("scripts/mixedlogit_scriptinteractions.R")

delfiles <- dir(path="modeloutput/",  pattern=".csv" ,full.names = T)

file.remove(delfiles)

```


The following chunk is also set to `eval=FALSE`. It estimates Latent Class models with 2 to 6 classes.
```{r estimate lcmodels, eval = FALSE}

map(2:6, function(x) {n_classes <<- x  
                       source("scripts/lcmodels/lc_body.R")
} )


```

The following chunks read in estimated models

```{r readinsavedWTPModel, eval=T, results='hide'}


delfiles <- dir(path="modeloutput/",  pattern="OLD" ,full.names = T,recursive = TRUE)

file.remove(delfiles)


  



modelpath <-list.files(path = "modeloutput/WTPSpace/",pattern = "rds$" , full.names = TRUE, recursive = TRUE)
models_tex <- list()
models <- list()


for (name in modelpath) {

t <- readRDS(name)
#twtp <- wtp("b_cost",attr = names(t$estimate) , modelname=t) # only relevant if models are not estimated in WTP space

models[[t$apollo_control$modelName]] <-t
models_tex[[t$apollo_control$modelName]] <-quicktexregapollo(t)

}

rm(t, name, modelpath)
  
 

```



```{r readinsavedPrefModel, eval=T, results='hide'}




modelpath <-list.files(path = "modeloutput/mixlogit_prefspace/",pattern = "rds$" , full.names = TRUE)

modelsWTP <- list()
modelsWTP_tex <- list()



for (name in modelpath) {

t <- readRDS(name)

if(any(t$apollo_draws$interNormDraws %in% "draws_cost")) mcost=TRUE else mcost=FALSE

print(mcost)

twtp <- wtp("mean_cost",attr = names(t$estimate) , modelname=t ,mediancost = mcost) # only relevant if models are not estimated in WTP space

models[[t$apollo_control$modelName]] <-t
models_tex[[t$apollo_control$modelName]] <-quicktexregapollo(t)
modelsWTP[[t$apollo_control$modelName]]<-twtp
modelsWTP_tex[[t$apollo_control$modelName]] <- quicktexregapollo(model=t,wtpest = twtp)

}

#rm(t,twtp, name, modelpath)
  
 

```


This chunk loads saved latent class models and prepares tables with coefficients and willingness to pay.

```{r loadlcmodels , eval=TRUE, results='hide'}

delfiles <- dir(path="modeloutput/lclogit",  pattern="OLD" ,full.names = T)

file.remove(delfiles)

modelpath <-list.files(path = "modeloutput/lclogit/",pattern = "*.rds" , full.names = TRUE)
lcmodels_tex <- list()
lcmodels     <- list()
lcmodelsWTP  <- list()
lcmodelsWTP_tex <- list()

for (name in modelpath) {

  t <- readRDS(name)

  twtp <- wtp_lc(modelname=t)
  
  lcmodels[[t$apollo_control$modelName]]     <-t
  lcmodels_tex[[t$apollo_control$modelName]] <-quicktexregapollo(t)
  lcmodelsWTP[[t$apollo_control$modelName]]  <- twtp

 for (class in 1:(length(t$LL0)-1)) {

    cl <- twtp[[class]]
    clet <- intToUtf8(96+class)



 lcmodelsWTP_tex[[t$apollo_control$modelName]][[paste0("Class", class)]] <-  quicktexregapollo(t,cl)

}


}

rm(t,twtp, name, modelpath)

```
</details>

# Appendix A: Individual Willingness to Pay Values

Based on the models presented in the main  text, the following present individual (conditional) willingness to pay values.

<details>
  <summary>Appendix A: Individual Willingness to pay values</summary>

```{r individual wtp, out.width="120%"}
library(purrr)
library(ggpubr)

# Read in individual WTP estimates 
individualWTP_notext <- readRDS("modeloutput/Individual_wtp/individualWTP_notext.rds")
individualWTP_ind <- readRDS("modeloutput/Individual_wtp/individualWTP_ind.rds")
individualWTP_env <- readRDS("modeloutput/Individual_wtp/individualWTP_env.rds")

# names of the attributes
attribute_names <- c("ASC", "EU Organic", "EU Organic + Bioverita", "Local", "Germany", "Spain", "Open-source/commons", "Price")
names(individualWTP_env)<- attribute_names 
names(individualWTP_ind)<- attribute_names 
names(individualWTP_notext)<- attribute_names 

# Define a function that takes a variable name as input and returns a ggplot object
plot_wtp <- function(var_name) {
  ggplot(data.frame(x = individualWTP_env[[var_name]]$post.mean), aes(x = x, col="Env")) +
    geom_density() +
    geom_density(data = data.frame(x = individualWTP_ind[[var_name]]$post.mean), aes(x = x, col="Ind")) +
    geom_density(data = data.frame(x = individualWTP_notext[[var_name]]$post.mean), aes(x = x, col="Notext")) +
    ggtitle(var_name) +
    xlab("WTP") +
    ylab("Density") +
    labs(col="Treatment") +
    theme(legend.position = c(0.15, 0.85),title = element_text(size=7))
}

# apply the function to each variable in individualWTP data frames
plot_list <- map(names(individualWTP_env), plot_wtp)


# plot densities 
ggarrange(plotlist = plot_list, nrow = 2, ncol = 4, common.legend = T, legend = "bottom")

```

```{r, include=F}
ggsave("Figure_revision/individualWTP.png", width = 16, height =9, dpi="print")
```
</details>

# Appendix B: Models separated by Season

Respondents were randomly assigned into either a winter or a summer scenario. The following output shows two mixed logit models in WTP space separated by winter and summer samples.

<details>
  <summary>Appendix B: Models separated by season</summary>

```{r, message=F}
library(tibble)
library(reshape2)
```


```{r season, eval=T, out.width="120%"}

mxl_season_compare <- as.data.frame(models$mixlog_season_winter$estimate)
mxl_season_compare[2] <- as.data.frame(models$mixlog_season_summer$estimate)

alpha = 0.1 # set confidence level 

mxl_season_compare$margin_of_error_w <- qnorm(1-alpha/2)*models$mixlog_season_winter$robse
mxl_season_compare$margin_of_error_s <- qnorm(1-alpha/2)*models$mixlog_season_summer$robse

mxl_season_compare <- rownames_to_column(mxl_season_compare, "Coefficent")

colnames(mxl_season_compare) <- c("Coefficent", "Estimate_winter", "Estimate_summer", "Margin_of_error_winter",
                                "Margin_of_error_summer")


mxl_melt_season <- melt(mxl_season_compare[1:3], id = "Coefficent")
mxl_melt_season$ME <- mxl_season_compare$Margin_of_error_winter
mxl_melt_season$ME[18:34] <- mxl_season_compare$Margin_of_error_summer


ggplot(data=mxl_melt_season, aes(x=Coefficent, y=abs(value), fill=variable)) +
  geom_bar(stat="identity",  position='dodge', width = 0.9) +
  geom_errorbar(aes(x=Coefficent, ymin=abs(value)-ME, ymax=abs(value)+ME), width=0.3, position=position_dodge(0.8)) +
  ylab("Absolute Value") +
  scale_x_discrete(guide = guide_axis(angle = 45)) +
  scale_fill_brewer(palette = 7, labels = c("Winter", "Summer"), name="Season") +
  theme(legend.position = c(0.9, 0.85))

```

```{r season table, results='asis'}
mixmodelsWTP_tex <- models_tex[c("mixlog_season_winter", "mixlog_season_summer") ]

coef_names <- c("ASC: Mean", "ASC: Standard deviation", "EU Organic: Mean", "EU Organic: Standard deviation", "EU Organic + Bioverita: Mean", "EU Organic + Bioverita: Standard Deviation", "Local: Mean", "Local: Standard deviation", "Germany: Mean", "Germany: Standard deviation", "Spain: Mean", "Spain: Standard deviation", "Open-source/commons: Mean", "Open-source/commons: Standard deviation", "Price: Mean", "Price: Standard deviation", "Ownership x Open-source vs. commons")

htmlreg(l = mixmodelsWTP_tex  ,
        custom.model.names = c("Winter", "Summer") ,  caption = "Results for mixed logit model for the two different seasons" , single.row = T  , custom.coef.names = coef_names,
        custom.note = "***p < 0.001; **p < 0.01; *p < 0.05 Standard Errors in Brackets" , center = TRUE, doctype = FALSE)
```


# Appendix C: Models separated by Tomato Type

The following shows mixed logit models in WTP space separated by tomato type. Before starting with the choice experiment, respondents could choose whether they purchase cherry tomatoes or flesh tomatos.

<details>
  <summary>Appendix C: Models separated by Tomato Type</summary>

```{r tomato_type, eval=T, out.width="120%"}


mxl_tomato_compare <- as.data.frame(models$mixlog_small_tomato$estimate)
mxl_tomato_compare[2] <- as.data.frame(models$mixlog_big_tomato$estimate)

alpha = 0.1 # set confidence level 

mxl_tomato_compare$margin_of_error_w <- qnorm(1-alpha/2)*models$mixlog_small_tomato$robse
mxl_tomato_compare$margin_of_error_s <- qnorm(1-alpha/2)*models$mixlog_big_tomato$robse

mxl_tomato_compare <- rownames_to_column(mxl_tomato_compare, "Coefficent")

colnames(mxl_tomato_compare) <- c("Coefficent", "Estimate_small", "Estimate_big", "Margin_of_error_small",
                                "Margin_of_error_big")


mxl_melt_tomato <- melt(mxl_tomato_compare[1:3], id = "Coefficent")
mxl_melt_tomato$ME <- mxl_tomato_compare$Margin_of_error_small
mxl_melt_tomato$ME[19:36] <- mxl_tomato_compare$Margin_of_error_big


ggplot(data=mxl_melt_tomato, aes(x=Coefficent, y=abs(value), fill=variable)) +
  geom_bar(stat="identity",  position='dodge', width = 0.9) +
  geom_errorbar(aes(x=Coefficent, ymin=abs(value)-ME, ymax=abs(value)+ME), width=0.3, position=position_dodge(0.8)) +
  ylab("Absolute Value") +
  scale_x_discrete(guide = guide_axis(angle = 45)) +
  scale_fill_brewer(palette = 13, labels = c("Small", "Big"), name="Tomato") +
  theme(legend.position = c(0.9, 0.85))

```




```{r tomato table, results='asis'}
mixmodelsWTP_tex <- models_tex[c("mixlog_small_tomato", "mixlog_big_tomato") ]

coef_names <- c("ASC: Mean", "ASC: Standard deviation", "EU Organic: Mean", "EU Organic: Standard deviation", "EU Organic + Bioverita: Mean", "EU Organic + Bioverita: Standard Deviation", "Local: Mean", "Local: Standard deviation", "Germany: Mean", "Germany: Standard deviation", "Spain: Mean", "Spain: Standard deviation", "Open-source/commons: Mean", "Open-source/commons: Standard deviation", "Price: Mean", "Price: Standard deviation", "ASC x Winter", "Ownership x Open-source vs. commons")

htmlreg(l = mixmodelsWTP_tex  ,
        custom.model.names = c("Small Tomato", "Big Tomato") ,  caption = "Results for mixed logit model for the two different tomato types" , single.row = T  , custom.coef.names = coef_names,
        custom.note = "***p < 0.001; **p < 0.01; *p < 0.05 Standard Errors in Brackets" , center = TRUE, doctype = FALSE)
```
</details>

# Appendix D: Latent Class Analysis

 In addition to the mixed logit models, we estimate latent class models with two to six classes. Latent class analyses can provide additional insights into the distribution of preferences by allocating respondents into different preference segments (classes). The following tables list the five estimated latent class models from two to six classes. A priori, the optimal number of classes is unknown. In our case, we present results from all five latent class models and discuss how they differ in terms of model fit, plausibility of results, and similarity to results from mixed and conditional logit models to determine a model we consider the best. 
 
Both, the five-class and a six-class model have convergence issues and standard errors could not be calculated. Therefore, we do not consider these two models in the further analysis. 
Regarding model fit, figure 7.1 shows BIC, AIC, and cAIC for the estimated latent class models. We observe a small improvement from a two-class model to a three-class model and a comparatively large improvement from the three-class model to the four-class model. We thus consider the four-class model as the best with respect to model fit. 

Another criterion for a well-specified and robust model is the extent to which the mean WTP values over all classes correspond to the mean values of the mixed and conditional logit models we have estimated. Table 14.1 in Appendix I overviews the mean (and median in case of the mixed logit model with a log-normally distributed price coefficient) WTP of estimated models. The table reveals that the two-class model has mean WTP values most similar to all other models. Also, the four-class model has reasonable mean WTP values yet they are significantly higher than in the mixed logit models. The three-class model has highly diverging mean WTP values compared to all other models. Hence, we consider the two-class and the four-class models as best. 
In both models and in all classes, all coefficients except the interaction coefficient with open source vs. commons are statistically significant. Both models indicate one class (34% in the two-class model, 19% in the four-class model) in which respondents are only willing to pay for tomatoes if the attributes take specific levels. More specifically, the WTP for a base version of the tomatoes (non-EU, not organic) is below zero and is only above zero if at least one of these attributes improves. Regarding the remaining shares, the four-class model differentiates further, while the two-class model summarizes the remaining heterogeneity in one class. This class (66%) has a comparatively high WTP for a base version (non-EU, non-organic, private ownership) of EUR 4.2. WTP for regional is EUR 2.3 and for Bioverita EUR 1.9. For open source/commons, it is EUR 0.4. The four-class model is more differentiated. Here, class 2 (15%) and class 3 (41%) have lower WTP values for the base level. The main difference between the classes is the WTP for the open source/commons attribute. While in class 2, WTP for this attribute is EUR 0.2, it is EUR 1.1 in class 3. This finding is especially relevant, as it indicates that there is a large share of respondents who do have a strong preference for open source/commons products, while others have no or weaker preferences. Finally, class 1 (25%) has a high WTP for the base product and a low yet positive WTP for the attributes. 

In total, the four-class model provides relevant information regarding the distribution of preferences in the sample. Class 1 describes high WTP for tomatoes, with low WTP for the attributes. In class 4, the levels of the attributes are decisive for WTP for tomatoes, and classes 2 and 3 have both WTP for tomatoes and for the attributes, and, most strikingly, they differ in the WTP for ownership. We leave it to the reader to dive deeper into the latent class analysis, using the code we provide on GitHub.
   
  
 
  
<details>
  <summary>Appendix D: Latent Class Analysis</summary>

```{r lc wtp , results="asis", include=TRUE, eval=TRUE} 

for(clno in names(lcmodelsWTP_tex)){

nclasses <-length(lcmodels[[clno]]$LL0)-1  
clshare <- round(100*sapply(lcmodels[[clno]][["unconditionals"]][["pi_values"]],mean), digits = 1  ) 
modelnames <- sprintf("Class %d",seq(1:nclasses ))

coef_names <- c("ASC", "EU Organic", "EU Organic + Bioverita", "Local", "Germany", "Spain",
                "Open-source/commons", "Ownership x Open-source vs. commons", "Delta")

print(
htmlreg(lcmodelsWTP_tex[[clno]] ,
       single.row = TRUE , custom.coef.names = coef_names, 
       custom.model.names = paste0(modelnames, "<br>  (" , clshare , "\\%)") ,
      # custom.model.names = paste0("\\specialcell{",modelnames, "\\\\  (" , clshare , "\\%)}") ,
       caption = paste0("WTP values of latent class model with " ,nclasses, " classes" ),
       #groups = list("WTP"=1:6, "Membership" = 7:length(lcmodelsWTP_tex[["lc2cl"]][["Class2"]]@coef.names) )
)
)
}       
 


```

```{r lc compare, include=F}
source("scripts/compare_lcmodels_correct.R")
```

```{r lc-graph, fig.cap = "Different Information Criteria for Latent Class Models"}
gof_lc_graph
```



# Appendix E: Conditional Logit models

We present conditional logit models which served as starting point for the analysis.

<details>
  <summary>Appendix E: Conditional Logit models</summary>

```{r clogittables, results='asis'}

clmodelsWTP_tex <- models_tex[c("clog_allsamples",  "clog_split_notext" , "clog_split_env"  ,  "clog_split_ind") ]

coef_names <- c("ASC", "ASC x Winter", "Price", "EU Organic", "EU Organic + Bioverita", "Local", "Germany", "Spain",
                "Open-source/commons", "Ownership x Open-source vs. commons")
htmlreg(l = clmodelsWTP_tex , 
        custom.model.names = c("Full Sample", "No Text", "Environment" , "Industry") , custom.coef.names = coef_names,  caption = "Results from Infotext Splits" , single.row = T  , 
        custom.note = "***p < 0.001; **p < 0.01; *p < 0.05 Standard Errors in Brackets" , center = TRUE, doctype = FALSE )


```
</details>

# Appendix F: Models used in Paper

The following chunks replicate the models used in the main text

<details>
  <summary>Appendix F: Models used in paper</summary>

```{r resultsmixedlogit, results='asis'}

mixmodelsWTP_tex <- models_tex[c("mixlog_allsamples",  "mixlog_split_notext" , "mixlog_split_env"  ,  "mixlog_split_ind") ]

coef_names <- c("ASC: Mean", "ASC: Standard deviation", "EU Organic: Mean", "EU Organic: Standard deviation", "EU Organic + Bioverita: Mean", "EU Organic + Bioverita: Standard Deviation", "Local: Mean", "Local: Standard deviation", "Germany: Mean", "Germany: Standard deviation", "Spain: Mean", "Spain: Standard deviation", "Open-source/commons: Mean", "Open-source/commons: Standard deviation", "Price: Mean", "Price: Standard deviation", "ASC x Winter", "Ownership x Open-source vs. commons")

htmlreg(l = mixmodelsWTP_tex  , custom.coef.names = coef_names,
        custom.model.names = c("Full Sample", "No Text", "Environment" , "Industry") ,  caption = "Results from Infotext Splits with mixed logit models" , single.row = T  , 
        custom.note = "***p < 0.001; **p < 0.01; *p < 0.05 Standard Errors in Brackets" , center = TRUE, doctype = FALSE )


```


# Appendix G: Pooled Model with Information Splits

To investigate treatment effects further, we estimate one model with all respondents and interaction terms with all attributes and the information splits.


<details>
  <summary>Appendix G: Pooled Model with Information Splits</summary>

```{r resultsmixedsplitint, results='asis'}


mixlogit_scriptint_tex <- models_tex[["mixlog_scriptint"]]


splitit <-function (select, listobj) {
  
  t<- listobj
  
cells <-grep(select,t@coef.names)  
  
t@coef.names <- t@coef.names[cells]
t@coef <- t@coef[cells]
t@se <- t@se[cells]
t@pvalues <- t@pvalues[cells]
return(t)  

}


intmodel_split <- purrr::map(c("mean","sd", "Info2","Info3"), splitit,mixlogit_scriptint_tex)

# change coefficient names for output table 
intmodel_split <- map(intmodel_split, ~ {
  if(length(.x@coef.names) > 7) {
    .x@coef.names <- c("asc", "eubio", "biover", "reg", "de", "esp", "eigent", "cost")
  } else {
    .x@coef.names <- c("asc", "eubio", "biover", "reg", "de", "esp", "eigent")
  }
  .x
})

coef_names <- c("ASC", "EU Organic", "EU Organic + Bioverita", "Local", "Germany", "Spain",
                "Open-source/commons", "Price")
htmlreg(l = c(intmodel_split[1], remGOF(intmodel_split[2:4])) ,  custom.model.names = c("Mean", "Standard deviation", "Interaction Info 2", "Interaction Info 3"), custom.coef.names = coef_names,
          caption = "Results from mixed logit model with interactions of information treatments" , single.row = T  , 
        
        custom.note = "***p < 0.001; **p < 0.01; *p < 0.05 Standard Errors in Brackets" , center = TRUE, doctype = FALSE )

```
</details>

# Appendix H: Z-Tests for Differences in Parameters between Information Splits

We used z-tests to check for differences in information treatment models.

<details>
  <summary>Appendix H: Z-Tests for differences in parameters between information splits</summary>

```{r ztest}



z.envXind <- apollo_ztest(models[["mixlog_split_env"]],models[["mixlog_split_ind"]])

z.envXnotext <- apollo_ztest(models[["mixlog_split_env"]],models[["mixlog_split_notext"]])


z.notextXind <-apollo_ztest(models[["mixlog_split_notext"]],models[["mixlog_split_ind"]])


apollo_ztest(models[["clog_split_env"]],models[["clog_split_ind"]])

apollo_ztest(models[["clog_split_env"]],models[["clog_split_notext"]])


apollo_ztest(models[["clog_split_notext"]],models[["clog_split_ind"]])


ztest.pvals <- z.envXind %>% 
  tibble::rownames_to_column() %>% 
  bind_cols(z.envXnotext,z.notextXind ) %>% 
  select(EnvXind = p_value...9 , EnvXnotext = p_value...17, notextXind = p_value...25)

rownames(ztest.pvals) <- coef_names <- c("ASC: Mean", "ASC: Standard deviation", "EU Organic: Mean", "EU Organic: Standard deviation", "EU Organic + Bioverita: Mean", "EU Organic + Bioverita: Standard Deviation", "Local: Mean", "Local: Standard deviation", "Germany: Mean", "Germany: Standard deviation", "Spain: Mean", "Spain: Standard deviation", "Open-source/commons: Mean", "Open-source/commons: Standard deviation", "Price: Mean", "Price: Standard deviation", "ASC x Winter", "Ownership x Open-source vs. commons")

kable(ztest.pvals) %>% kable_styling()

#flextable(ztest.pvals,) %>% colformat_double(digits = 3) %>% save_as_docx(path = "tables/ztest.docx")


```
</details>


# Appendix I: Model with Familiarity Interactions

As we expect that people who are familiar with the attributes are more likely to have a high WTP for these attributes, we estimate a model with interactions of the respective attribute and the stated familiarity question.

<details>
  <summary>Appendix I: Model with familiarity interactions</summary>

```{r fam int, results='asis'}

mixmodelsWTP_tex <- models_tex[c("mixlog_int_tab4")]

coef_names <- c("ASC: Mean", "ASC: Standard deviation", "EU Organic: Mean", "EU Organic: Standard deviation", "EU Organic + Bioverita: Mean", "EU Organic + Bioverita: Standard Deviation", "Local: Mean", "Local: Standard deviation", "Germany: Mean", "Germany: Standard deviation", "Spain: Mean", "Spain: Standard deviation", "Open-source/commons: Mean", "Open-source/commons: Standard deviation", "Price: Mean", "Price: Standard deviation", "ASC x Winter", "Ownership x Open-source vs. commons", "EU Organic: Familiarity interaction", "EU Organic + Bioverita: Familiarity interaction", "Open-source/commons: Familiarity interaction")

htmlreg(l = mixmodelsWTP_tex  , custom.coef.names = coef_names,
        custom.model.names = c("Familiarity interactions") ,  caption = "Results from familiarityinteractions model" , single.row = T  , 
        custom.note = "***p < 0.001; **p < 0.01; *p < 0.05 Standard Errors in Brackets" , center = TRUE, doctype = FALSE )


```

</details>

# Appendix J: Models in Preference Space 

We also estimated mixed logit models in preference space. These models were also considered as the main models in the main text.

<details>
  <summary>Appendix J: Models in preference space</summary>

```{r pref space, results='asis'}

mixmodelsWTP_tex <- models_tex[c("mixlog_fix_price", "mixlog_pref_space")]

coef_names <- c("ASC: Mean", "ASC: Standard deviation", "EU Organic: Mean", "EU Organic: Standard deviation", "EU Organic + Bioverita: Mean", "EU Organic + Bioverita: Standard Deviation", "Local: Mean", "Local: Standard deviation", "Germany: Mean", "Germany: Standard deviation", "Spain: Mean", "Spain: Standard deviation", "Open-source/commons: Mean", "Open-source/commons: Standard deviation", "ASC x Winter", "Price: Mean", "Ownership x Open-source vs. commons", "Price: Standard deviation")

htmlreg(l = mixmodelsWTP_tex  ,
        custom.model.names = c("Fixed price", "Log-normal price") ,  caption = "Results from models in preference space" , single.row = T  , custom.coef.names = coef_names,
        custom.note = "***p < 0.001; **p < 0.01; *p < 0.05 Standard Errors in Brackets" , center = TRUE, doctype = FALSE )


``` 
 
```{r pref space_wtp, results='asis'}

mixmodelsWTP_tex <- modelsWTP_tex

coef_names <- c("ASC", "EU Organic", "EU Organic + Bioverita", "Local", "Germany", "Spain",
                "Open-source/commons", "ASC x Winter", "Ownership x Open-source vs. commons")

htmlreg(l = mixmodelsWTP_tex  ,
        custom.model.names = c("Fixed price", "Log-normal price") ,  caption = "Results from models in preference space as WTP values" , single.row = T  , custom.coef.names = coef_names,
        custom.note = "***p < 0.001; **p < 0.01; *p < 0.05 Standard Errors in Brackets" , center = TRUE, doctype = FALSE , omit.coef = "^sd_") 


```  
</details>


# Appendix K: Comparison of WTP across all Models

<details>
  <summary>Appendix K: Comparison of WTP across all models</summary>

We provide a table that compares mean and median (in case of log-normal cost coefficient in preference space) WTP across all models.

```{r}

source("scripts/function_lc_avg.R")
wtp <- list()
for (z in 2:6) {
  num_classes <- z
  wtp[[z-1]] <- calculate_avg_lc_wtp(lcmodels[[paste0(z, " Classes")]], lcmodelsWTP[[paste0("lc_", z, "class")]], num_classes)
}

lc_av_wtp <- as.data.frame(wtp)
colnames(lc_av_wtp) <- names(lcmodels)
rownames(lc_av_wtp) <- c("ASC", "EU Organic", "EU Organic + Bioverita", "Local", "Germany", "Spain", "Ownership","Ownership x Open-source vs. commons", "Delta")
lc_av_wtp <- lc_av_wtp[1:8, ]

# add clogit

clogit <- as.data.frame(models$clog_allsamples$estimate)
clogit$Coefficient <- rownames(clogit)
colnames(clogit) <- c("wtp", "Coefficient")
clogit <- clogit %>% filter(Coefficient!="ascwinter"& Coefficient!="cost")


#add pref space models 
fx_wtp <-  as.data.frame(modelsWTP$mixlog_fix_price)
fx_wtp$Coefficient <- rownames(fx_wtp)
fx_wtp <- fx_wtp %>% filter(!grepl("^sd", Coefficient)) %>% filter(Coefficient!="ascwinter")

log_wtp <- as.data.frame((modelsWTP$mixlog_pref_space))
log_wtp$Coefficient <- rownames(log_wtp)
log_wtp <- log_wtp %>% filter(!grepl("^sd", Coefficient)) %>% filter(Coefficient!="ascwinter")

all_wtp <- as.data.frame(models$mixlog_allsamples$estimate)
all_wtp$Coefficient <- rownames(all_wtp)
colnames(all_wtp) <- c("wtp", "Coefficient")
all_wtp <- all_wtp %>% filter(!grepl("^sd", Coefficient)) %>% filter(Coefficient!="ascwinter" & Coefficient!="mean_cost")

lc_av_wtp$Fixed_Price <- fx_wtp$wtp
lc_av_wtp$Log_normal_Price <- log_wtp$wtp
lc_av_wtp$clogit <- clogit$wtp
lc_av_wtp$Full_sample_Wtp <- all_wtp$wtp





``` 



```{r lc-pre-all}

kable(lc_av_wtp, digits = 2, caption = "Overview of mean WTP values for the different models" ,
        col.names = c(names(lc_av_wtp[1:5]), "Fixed Price", "Log Normal Price", "Conditional Logit" , "Model used in manuscript") ) %>%
  kable_styling(full_width = FALSE) %>% 
  add_header_above(c("","Latent class models"=5, "Preference Space models"=2, "WTP Space Models" =2)) %>% 
  add_footnote("All values are mean values except for the mixed logit model with a log-normally distributed price coefficient, where we report the medians. For the latent class models, we use the weighted averages over all classes.", notation="symbol")


```

  
</details>


# Appendix L: Socio-demographic Comparison of the Information Splits

<details>
  <summary>Appendix L: Socio-demographic comparison of the information splits</summary>
  
```{r}

socio <- database2 %>% distinct(RID, .keep_all=TRUE) %>% 
                       mutate(Age = q21 + 14 ,
                              info_text =case_when(info_text ==1 ~ "No Text", 
                                        info_text ==2 ~ "Environment", 
                                        TRUE ~ "Industry")) %>%
                       rename("HH_Size" = q24) %>% 
                       select(Income, Female, Age, HigherEdu, HH_Size,info_text) 


summary_socio <-socio %>%   group_by(info_text) %>% 
  summarize(across(everything(), ~round(mean(.,rm.na=TRUE),2) ))

#treatment_list <- map(set_names(treatment), ~ filter(socio, info_text == .x))

kable(summary_socio,digits = 2, caption = "Overview of the main socio-demographic variables for the different            treatment groups" , col.names = c("Treatment Group", "Income", "Share of Females" , "Age", "Higher Education", "Household Size")) %>%
       kable_styling(full_width = FALSE) %>% 
       add_footnote("The table reports the means of the variables of the different information split samples. Income was defined as a categorical variable in the survey.", notation="symbol")



```



```{r sociotestcomp1}





# compute statistical tests
# 
p_values <- c()
p_values[1] <- kruskal.test(Income ~ info_text, data=socio)$p.value
p_values[2] <-chisq.test(socio$Female, socio$info_text)$p.value
p_values[3] <-rstatix::anova_test(Age~ info_text, data = socio)$p
p_values[4] <-chisq.test(socio$HigherEdu, socio$info_text)$p.value
p_values[5] <-rstatix::anova_test(HH_Size~ info_text, data= socio)$p
# 
 variables <- c( "Income", "Share of Females" , "Age", "Higher Education", "Household Size")
 tests <- c("Kruskal-Wallis Test", "Chi-Squared Test", "One-way ANOVA Test", "Chi-Squared Test", "One-way ANOVA Test")
# 
 p_values <- bind_cols( variables,  tests, p_values)
 colnames(p_values) <- c("Variable", "Test", "P-Value")
# 
# 
 kable(p_values, digits=3, caption = "Results of the test for differences among the treatment groups") %>%  kable_styling(full_width = FALSE) %>% 
    add_footnote("The null hypothesis of the tests is that there exist no differences in the means of the respective variables between the different groups.", notation="symbol")
      

```



```{r sociocomp1}

socio <- database2 %>% distinct(RID, .keep_all=TRUE) %>% 
                       mutate(Age = q21 + 14 ,
                              oc =case_when(oc ==0 ~ "Commons Framing", 
                                        oc ==1 ~ "Open Source Framing")) %>%
                       rename("HH_Size" = q24) %>% 
                       select(Income, Female, Age, HigherEdu, HH_Size,oc) 


summary_socio <-socio %>%   group_by(oc) %>% 
  summarize(across(everything(), ~round(mean(.,rm.na=TRUE),2) ))

#treatment_list <- map(set_names(treatment), ~ filter(socio, info_text == .x))

kable(summary_socio,digits = 2, caption = "Overview of the main socio-demographic variables for the open source and commons split samples" , col.names = c("Treatment Group", "Income", "Share of Females" , "Age", "Higher Education", "Household Size")) %>%
       kable_styling(full_width = FALSE) %>% 
       add_footnote("The table reports the means of the variables of the open source and commons split samples. Income was defined as a categorical variable in the survey.", notation="symbol")

```




```{r sociocomptest1}


# compute statistical tests
# 
p_values <- c()
p_values[1] <- kruskal.test(Income ~ oc, data=socio)$p.value
p_values[2] <-chisq.test(socio$Female, socio$oc)$p.value
p_values[3] <-rstatix::anova_test(Age~ oc, data = socio)$p
p_values[4] <-chisq.test(socio$HigherEdu, socio$oc)$p.value
p_values[5] <-rstatix::anova_test(HH_Size~ oc, data= socio)$p
# 
 variables <- c( "Income", "Share of Females" , "Age", "Higher Education", "Household Size")
 tests <- c("Kruskal-Wallis Test", "Chi-Squared Test", "One-way ANOVA Test", "Chi-Squared Test", "One-way ANOVA Test")
# 
 p_values <- bind_cols( variables,  tests, p_values)
 colnames(p_values) <- c("Variable", "Test", "P-Value")
# 
# 
 kable(p_values, digits=3, caption = "Results of the test for differences among open source and commons split samples") %>%  kable_styling(full_width = FALSE) %>% 
    add_footnote("The null hypothesis of the tests is that there exist no differences in the means of the respective variables between the different groups.", notation="symbol")
      



```

